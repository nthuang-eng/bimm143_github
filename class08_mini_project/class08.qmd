---
title: "Class 08: Breast Cancer Mini Project"
author: "Nathalie Huang (PID: A19134713)"
format: pdf
toc: TRUE
---

## Background

In today's class we will be employing all the R techniques for data and analysis that we have learned thus far - including the machine learning methods of clustering and PCA - to analyze real breast cancer biopsy data.

## Data Import

The data is in CSV format:

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
```

wee peek at the data
```{r}
head(wisc.df, 3)
```
> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.df)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
length(grep("_mean", colnames(wisc.df)))
```

We need to remove the `diagnosis` column before we do any further analysis of this dataset - we don't want to pass this to PCA etc. We will save it as a separate wee vector that we can use later to compare our findings to those of experts...

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df$diagnosis
```

## Principal Component Analysis (PCA)

The main function in base R is called `prcomp()` we will use the optional argument `scale=TRUE` here as the data columns/features/dimensions are very different scales in the original data set.

```{r}
wisc.pr <- prcomp(wisc.data, scale=T)
```

```{r}
attributes(wisc.pr)
```

```{r}
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)?

0.4427 = 44.27%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 principal components

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 principal components

```{r}
biplot(wisc.pr)
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The biplot is very cluttered and difficult to interpret. With so many observations and variables, the points and variable arrows overlap heavily, making it hard to see patterns or understand how variables contribute to the principal components.

```{r}
library(ggplot2)

ggplot(wisc.pr$x) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```
> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC3, col=diagnosis) +
  geom_point()
```

## Variance explained

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- pr.var / sum(pr.var)
```

```{r}
plot(c(1, pve), 
     xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), 
     type = "o")
```

```{r}
barplot(pve, 
        ylab = "Percent of Variance Explained",
        names.arg = paste0("PC", 1:length(pve)), 
        las = 2, 
        axes = FALSE)
axis(2, at = pve, labels = round(pve, 2) * 100)
```

#install.packages("factoextra")
```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

```{r}
wisc.pr$rotation
```

The loading for concave.points_mean on the first principal component is relatively large and positive, showing that this feature contributes strongly to PC1. While a few other features like concavity_mean, area_mean, and perimeter_mean have similarly large loadings, none contribute substantially more than concave.points_mean. This indicates that PC1 is mainly influenced by tumor shape and size features.

## Hierarchical clustering

The goal of this section is to do hierarchical clustering of the original datato see if there is any obvious grouping into malignant and benign clusters.


In short, these results are not good!

First we will scale our `wisc.data` then calculate a distance matrix, then pass to `hclust()`:
```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method = "complete")
```

> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h = 15, col = "red", lty = 2)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

My favorite method for this dataset is ward.D2, because it creates more compact clusters and minimizes variance within each cluster. It separates malignant and benign samples better than single, complete, or average linkage, producing clusters that correspond more closely to the diagnosis.

## Combining methods

The idea here is that I can take my new variable (i.e. the scores on the PCs `wisc.pr$x`) that are better descriptors of the data-set than the original features (i.e. the 30 columns in `wisc.data`) and use these as a basis for clustering.

```{r}
pc.dist <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(pc.dist, method= "ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(diagnosis)
```

I can now run `table()` with both my clustering `grps` and the expert `diagnosis`

```{r}
table(grps, diagnosis)
```

> Q13. How well does the newly created hclust model with two clusters separate out the two “M” and “B” diagnoses?

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)

table(wisc.pr.hclust.clusters, diagnosis)
```
> Q14. How well do the hierarchical clustering models you created in the previous sections (i.e. without first doing PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.hclust.clusters and wisc.pr.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)

table(wisc.hclust.clusters, diagnosis)
```

Our cluster "1" has 179 "M" diagnosis
or cluster "2" has 333 "B" diagnosis

179 TP
24 FP
333 TN
33 FN

Sensitity: TP/(TP+FN)
```{r}
179/(179+33)
```

Specificity: TN/(TN+FP)
```{r}
333/(333 + 24)
```

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC2) +
  geom_point(col=grps)
```

## Prediction

We can use our PCA model for prediction of new un-seen cases:

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q16. Which of these new patients should we prioritize for follow up based on your results?

New patients assigned to cluster 1 are most likely malignant based on the PCA + hierarchical clustering model.